% #############################################################################
% This is Chapter 3
% !TEX root = ../main.tex
% #############################################################################
% Change the Name of the Chapter i the following line
\fancychapter{Case Study}
\cleardoublepage
% The following line allows to ref this chapter
\label{chap:architecture}


In the last chapter we presented some of the most important models in the literature regarding the prediction of power consumption in buildings. In this chapter, we present in detail the challenge proposed by \ac{EDP}, and the procedure carried out throughout the development of this thesis.

\section{Problem statement and framework}\label{chap3:sec:problem_statement}

The proposed objective for this work is to create a system capable of computing predictions regarding the available power in a building, 5, 10 and 15 minutes ahead. Since all the experiment is directly related to the behavior of a specific building, it makes sense to present the building and its characteristics. Then, it is important to describe the whole procedure carried out to be able to produce those three forecasts.

\section{The building}\label{chap3:sec:building}

The building provided by \ac{EDP} for the development of this research has a total useful area of 39,801$m^2$ distributed in 5 major categories: office area, parking area, technical area, gymnasium area and bar. The first two areas represent 89\% of the total area of the building. In Figure \ref{consedp} the diagram shows the distribution of energy consumption by type of end use in each area of the building, for the year of 2016.

\begin{figure}[h!]
    \centering
    \begin{center}
    \includegraphics[width=1\textwidth]{Images/ConsumoEDP.png}
    \caption{Energy consumption for the building typology(s) with the highest consumption (2016).}
    \label{consedp}
    \end{center}
\end{figure}
In Figure \ref{consedp}, it can be seen that the \ac{HVAC} system represents almost 50\% of the consumption in the office zone. The use of the \ac{HVAC} system is directly related to the external weather conditions. Since 44\% of the consumption is related to the office cooling process, it can be deduced that in the region where this building is located, in warmer years, the consumption of the office area is higher, contributing to an increase in the total consumption of the building. It can also be seen that only 11\% of consumption is due to the lighting of the office area. On the other hand, 42\% of the consumption in the office area comes from all other factors such as energy plugs. The technical zones represent about 14\% of the total consumption of the building although they represent a small portion of the total area of the building. Most of the consumption performed in these areas is identified as "other" and concerns servers and technical material of high energy consumption, hence the enormous representativeness of these zones in the total consumption of the building. Parking lots also represent a significant portion of the total consumption of the building. It should be noted that 60\% of consumption is also identified as "other" in this area. This portion derives mainly from the electric vehicle charging procedure, that is carried out in the building's garage, managed by the \ac{EVCS}, addressed in Chapter \ref{chap:intro}. This system consists of a total of 13 charging stations for \ac{EV}s, and falls into the category of controllable consumption, i.e., the system can control the power used in vehicle charging, based on the priority level of each vehicle (such as the percentage of full battery charged), and the total power available in the building. The contribution of this thesis falls exactly in this category. Although the system has instantaneous information about the state of available power in the building, it would be of great use to have information regarding the future state of available power. In this sense, the work developed in this dissertation aims to respond to this need, contributing to the optimization of the current system.

Modeling energy consumption behaviour and identifying patterns can be crucial both to minimize power usage, and to develop new methodologies to optimize resources. Figure \ref{consumption} illustrates the energy consumption profile of the building of \ac{EDP} in Lisbon, Portugal, during the month of February 2020. The profile illustrated in Figure \ref{consumption} shows a movement that one can identify as a trend. The data presented provides power consumption details for 4 weeks and 4 weekends. The behaviour of the profile tends to be cyclical, with a 7 day period. 
\begin{figure}[h!]
    \centering
    \begin{center}
    \includegraphics[width=1\textwidth]{Images/power_consumption.png}
    \caption{EDP's building's a) consumption.}
    \label{consumption}
    \end{center}
\end{figure}


Variables that present a trend are more easily examined since it is sometimes possible to associate a cause with their cyclical behavior. By identifying this cause, it becomes not only easier to control its current behavior, but also to predict its future behavior. 

As far as power production is concerned, the building is equipped with a total of 500$m^2$ of \ac{PV} panels area on its roof. The installed panels have an efficiency rounding 15\%, representing a total of 70 kWp (kilowatt "peak") of solar generation. Although this kind of production is not controllable, since it depends solely on weather conditions, there are several relationships that can be established by having access to meteorological information. The increase in production generated by the \ac{PV} panels, directly represents an increase in the available power to consume. This factor then becomes an important indicator in the development of any forecasting system, since its behaviour directly influences the the amount of power available to charge \ac{EV}s. In Figure \ref{production}, one may see the power production profile for the same building and time-interval as Figure \ref{consumption}. The \ac{PV} production behaviour is also cyclical but presents a daily period due to the fact that solar radiation is not influenced by the occupancy rate of the building, so it has no direct relation with the day of the week, but with the hour of the day instead.

\begin{figure}[h!]
    \centering
    \begin{center}
    \includegraphics[width=1\textwidth]{Images/power_production.png}
    \caption{EDP's building's a) production.}
    \label{production}
    \end{center}
\end{figure}


Both power consumption and power production are vital information to have in order to predict power available at each time in the building. With this information, there are multiple innovative ideas that can be implemented with the goal of having a more sustainable and power-optimized building.

\section{Proposed variable to predict}\label{chap3:sec:variable_to_predict}

The available power is a key factor because it is the indicator provided to the \ac{EVCS} that enables it to proceed with the optimization of the distribution of the available power by the \ac{EV}s present in the building's garage, which need to be charged.

The power available is not a measured variable, it is a calculated variable. When it comes to computing it, one question then arises: Should one compute the available power and then predict it, or should one predict both consumed power and produced solar power, and just then perform the calculations to find the available power? Figure \ref{avsol} represents two possible solutions for this problem. The first solution consists in creating a model that has as input, besides the other variables, two variables: consumption power and solar production. This model will have as output a forecast for these two variables, and only then is the available power calculated. The second solution is to compute the available power before inputting the data into the model, which causes a reduction in the number of inputs and outputs in the model. 

\begin{figure}[h!]
    \centering
    \begin{center}
    \includegraphics[width=1\textwidth]{Images/Available.png}
    \caption{Model solutions.}
    \label{avsol}
    \end{center}
\end{figure}

Comparing the two proposed hypotheses, solution 1 presents the advantage of predicting two different variables allowing better data manipulation, offering more flexibility. Since the model produces separate results for the consumption and production variables, the amount of possible uses for this data is greater than the amount of possibilities provided by solution 2. On the other hand, solution 2 presents, first of all, the advantage of having a much simpler architecture which makes the user's work easier, and secondly, the main advantage is that it requires the model to predict only one variable instead of two. This factor is quite relevant, namely regarding the time it takes the system to produce forecasts, and also the computational capacity that is required from the hardware to allow the prediction of two variables simultaneously. The prediction of two variables with the same model also has the disadvantage of making it difficult to evaluate the behavior of the proposed solution, since a specific model can be very good for predicting one of the variables and very bad for the other. 
Taking all these factors into account, the second approach was then chosen. Thus, simplifying the user’s work, allowing to produce results quicker, and facilitating the process of choosing a model that produces a good solution for a single variable.


In Figure \ref{Max_cons_prod}, the reader may see the graph for the power profile of \ac{EDP}’s building, for February 2020, where the blue line is the power consumption of the building, the orange line the solar power production, and the red line the power of the electrical grid made available to this specific building.


\begin{figure}[h!]
    \captionsetup[subfigure]{position=b}
    \centering
    \label{fig:ap}
    \subcaptionbox{\label{Max_cons_prod}}{\includegraphics[width=1\linewidth]{Images/Max_cons_prod.png}}
    \subcaptionbox{ \label{available_power}}{\includegraphics[width=1\linewidth]{Images/Available_power.png}}
    \caption{EDP's building power trends a) grid capacity, consumption and production b) available power.}
\end{figure}


By analyzing Figure \ref{Max_cons_prod}, one can verify that, compared to the consumption presented by the building, the \ac{PV} panels do not have enough capacity for the building to be self-sustainable. That is to say, that solar power is far from being enough to maintain the regular operation of the building. One can conclude that the influence of the power generated by the \ac{PV} panels on the available power of the building is quite limited. However, in order to make a correct evaluation, it is a parameter that must be taken into account. The value of the available power is a computed metric, given by:

\begin{equation}
   P_{available} = P_{grid} + P_{solar} - P_{consumption},
   \label{available}
\end{equation}

where $P_{grid}$ is the maximum power that is made available to the building, provided by the electrical grid, which in this particular case is around 1.2 MW, $P_{solar}$ is the active solar power produced by the \ac{PV} system, and $P_{consumption}$ denotes the the current power consumption of the building. In Figure \ref{available_power}, the available power profile is shown for the same time interval as Figure \ref{Max_cons_prod}. Like the previous variables, it shows an equally cyclical behavior. It is also relevant to introduce the concept of stationarity. A stationary time series is one whose statistical properties such as the mean and the variance are constant over time. Most statistical forecasting methods are based on the assumption that the time series to be forecast is stationary or approximately stationary. By examining the graph presented in Figure \ref{available_power}, it can be seen that the variable to be predicted has, at least a locally stationary behaviour, preserving its statistical properties over time.

Taking into account all the factors mentioned, it was then decided to choose solution 2, in which the value of the available power is calculated beforehand, and is then used as input for the predictive model. This solution was chosen because it is a simpler architecture and requires less computational capabilities, since the model produces forecasts for only one variable, and not for two. Furthermore, the flexibility offered by forecasting the two variables separately is irrelevant in the context of the problem, since for the proposed objective, it is not necessary to differentiate the consumption forecast from the production forecast. 

\section{Data}\label{chap3:sec:data}

In this section, we describe the datasets used in the development of this work, and also detail the data preprocessing applied to the datasets in order to have the necessary formatting to be used during the experiment.

\subsection{Data description}\label{chap3:subsec:data_description}

In the day to day of a building, there are many factors that influence its consumption and, consequently, the available power. In Chapter \ref{chap:background}, some examples of work developed with the objective of predicting the energy consumption of a building were mentioned. In Appendix B, Table \ref{table1}, it is possible to verify that generally, two categories of data are used in this kind of applications: data that concerning the behavior of the building, and climatic data that characterizes the surrounding environment of the building.

Regarding the energy behavior of the building, \ac{EDP} provided two different datasets that present information between January 25, 2020 and September 30, 2020, totaling 8 months and 5 days of data, with a granularity of 5 seconds. The first dataset includes historical data concerning the power consumption of the building, and the second dataset includes historical data regarding the solar power production generated by the \ac{PV} panels installed on the top of the building. It is also relevant to mention that the production data provided showed some gaps, resulting from sensor reading failures. This aspect implies the need for some automatic mechanisms to complete the missing data, which are described below.

Regarding climate data, \ac{FCUL} provided two different datasets that present information between January 25, 2020 and June 15, 2020, totaling 4 months and 21 days of data, with a granularity of 1 minute. The first dataset includes meteorological data of the geographical location of the building, and the second includes data with respect to solar radiation exerted on the geographical location of the building. Neither of the two datasets had temporal flaws.

In Table \ref{table2}, there is a summary of the key points of the available data. The description of the variables for each of the datasets provided can be found in Appendix \ref{chapter:appendixE}, Table \ref{tab:available_variables}.

\begin{table}[htbp]
  \centering
  \caption{Summary of the datasets used.}
    \begin{tabular}{l|c|c|c|c|}
    \multicolumn{1}{c|}{\textbf{Provider}} & \multicolumn{2}{c|}{\textbf{EDP}} & \multicolumn{2}{c|}{\textbf{FCUL}} \\
    \midrule
    period & \multicolumn{2}{c|}{8 months and 5 days} & \multicolumn{2}{c|}{4 months and 21 days} \\
    beginning & \multicolumn{2}{c|}{25-01-2020} & \multicolumn{2}{c|}{25-01-2020} \\
    beginning & \multicolumn{2}{c|}{30-09-2020} & \multicolumn{2}{c|}{15-06-2020} \\
    \midrule
    Dataset & Consumption & Production & Meteorological & Radiation \\
    \# days & 249 & 249 & 142 & 142 \\
    \# samples & 4302720 & 4302720 & 2453760 & 2453760 \\
    \# variables & 8 & 7 & 22 & 28 \\
    \end{tabular}%
  \label{table2}%
\end{table}%


As can be seen through the analysis of Table \ref{table2}, the data provided does not portray the same time frame nor have the same granularity. It is then necessary to proceed to some data treatment in order to obtain the data in the desired form.

\subsection{Data completion}\label{chap3:subsec:data_completion}

The data completion consists of a process in which the data made available is formatted in such a way that it meets the essential criteria to enable its introduction in the models. During the development of the thesis, this was the most time consuming process. The data treatment is an extremely demanding process, which is illustrated in the diagram of Figure \ref{datatreatment}.

\begin{figure}[h!]
    \centering
    \begin{center}
    \includegraphics[width=1\textwidth]{Images/Data.png}
    \caption{Data treatment process.}
    \label{datatreatment}
    \end{center}
\end{figure}

In Phase 1, the production data (Prod.) is incomplete due to failure of the system responsible for acquiring and storing this indicator. There is also a difference, both in granularity and period available, between the data provided by \ac{EDP} and the data provided by \ac{FCUL}. It is then necessary, first of all, to equal the granularity of all four datasets. In order to do that, a function was applied to reduce the granularity of the production (Prod.) and consumption (Cons.) datasets, forcing the frequency of both datasets to one minute. The function applies an arithmetic mean given by 

\begin{equation}
     A={\frac {1}{n}}\sum _{i=1}^{n}a_{i}={\frac {a_{1}+a_{2}+\cdots +a_{n}}{n}},
\label{amean}
\end{equation}

where $A$ represents the value of the final measure with frequency of 1 minute, and $n$ represents the number of nodes between that minute range that will be converted to a single value. We then reached Phase 2, where the four datasets have the same granularity. Then, the datasets corresponding to the meteorological data \textit{Meteo.}, radiation data \textit{Rad.} and production data \textit{Prod.} are concatenated. As a result of this process (Phase 3), the dataset \textit{Solar} is formed, which aggregates all the information regarding climate data and solar production data over a period of 4 months and 21 days. The reason behind this phenomenon is that all the production days for which there is no direct correspondence in the meteorological data \textit{Meteo.} and radiation data \textit{Rad.} datasets are dropped. 

As mentioned before, the data regarding the solar production \textit{Prod.} presented some temporal flaws. In order to solve the problem, two solutions were found. For time gaps of less than 30 minutes (Phase 4), a quadratic interpolation was applied. As an example, given any three points $(x_0, f(x_0))$, $(x_1, f(x_1))$ and $(x_2, f(x_2))$, the polynomial that interpolates the three points is given by

\begin{equation}
\begin{split}
     & f_2(x)=b_0+b1(x-x_0)+b_2(x-x_0)(x-x1),\\
     & where:\\
     & b_0=f(x_0),\\
     & b_1=f[x_0,x_1]=\frac{f(x_1)-f(x_0)}{x_1-x_0},\\
     & b_2=f[x_0,x_1,x_2]=\frac{\frac{f(x_2)-f(x_1)}{x_2-x_1}-\frac{f(x_1)-f(x_0)}{x_1-x_0}}{x_2-x_0}.\\
\end{split}
\label{poly}
\end{equation}


In order to exemplify the evolution of the incomplete dataset construction, in the chart of Figure \ref{int0} one can observe the solar power generated measured by the sensors on February 24, 2020. In red, is the portion of data that was measured, corresponding to the Solar dataset in Phase 3 represented in Figure \ref{datatreatment}. After the interpolation, to the original dataset are added the points represented in blue, which represent measurement failures of less than 30 minutes, resulting from the interpolation procedure explained before. At the end of Phase 4, the dataset is then composed of the data points represented in blue plus the points represented in red.



\begin{figure}[h!]
    \centering
    \begin{center}
    \includegraphics[width=1\textwidth]{Images/int0.png}
    \caption{Data treatment process.}
    \label{int0}
    \end{center}
\end{figure}

For time failures of over 30 minutes, the dataset was completed with theoretically computed values, since the interpolation for large temporal failures does not present the expected behavior.
Based on the equation of the sun's position in the sky throughout the year, the maximum amount of solar insolation on a surface at a particular tilt angle can be calculated as a function of latitude and day of the year\cite{solar0}. In order to determine the direct component of solar radiation in $kW/m^2$, one used


\begin{equation}
     I_D = I_0*0.7^{(AM^{0.678})},
\label{solar}
\end{equation}

where $I_0$ is the solar intensity external to the Earth's atmosphere that is approximately $1.353 kW/m^2$, $0.7$ represents the overall attenuation of the atmosphere, $AM = \frac{1}{cos\theta}$ is the airmass and $\theta$ is the zenith angle (90° minus the altitude) of the sun. This formula produces an optimal result when there are no clouds. Multiplying the resulting value by the total area of panels in the building and taking into account their positioning efficiency, a theoretical value of power generated by the \ac{PV} panels is obtained, in an ideal scenario. In Phase 5, all missing readings were replaced by the calculated theoretical value, as can be seen in Figure \ref{int0} in green. This way, the Solar dataset was completed. It is possible to verify that the point of change between the theoretical data (the green one) and the original data (the red one) is somewhat sudden, because the green signal considers an ideal scenario and the red signal represents real measurements (affected by clouds, cranes, etc.). The sum of the green, blue and red signals, result in a complete signal for the solar active power generated by the \ac{PV} panels. 


The completion process of the \textit{Prod.} dataset could have been done before joining it with the other datasets (\textit{Meteo.} and \textit{Rad.}) in the Solar dataset, but it was decided to first reduce the size of the total set from 8 months and 5 days to 4 months and 21 days (Phase 2 $\to$ 3), and only then, with the \textit{Solar} dataset already created, perform the necessary computations to complete the missing data. The two completion procedures described before are computationally expensive. Therefore reducing the size of the dataset to be completed to half of its initial size has a major impact on the running time of the whole process. It is important to emphasize that, although the Phases 4 and 5 present in Figure \ref{datatreatment} represent the process of interpolation and computation of the theoretical solar value regarding the whole Solar dataset. These procedures were applied exclusively to the features concerning production, since these were the only ones that were incomplete until then.



In Phase 6 the \textit{Solar} and \textit{Cons.} datasets were concatenated, thus reducing the dataset time interval to a final result of 4 months and 21 days.

\subsection{Data partition}\label{chap3:subsec:data_partition}

In order to study a certain algorithm, it is necessary to have access to past data to train the model and then evaluate its performance. The available data is key point when it comes to developing predictive models. Usually, in \ac{ML}, to study a certain model, the dataset should be divided into three sets as can bee seen in Figure \ref{division}. 

\begin{figure}[h!]
    \centering
    \begin{center}
    \includegraphics[width=1\textwidth]{Images/division.png}
    \caption{Dataset division.}
    \label{division}
    \end{center}
\end{figure}

The first set is known as a training set, and consists of a dataset used to feed the model with examples in order to fit the hyperparameters, such as the weights of connections between neurons in \ac{ANNs}. As the name implies, the training set is suitable to train the model, progressively adapting the model to its intended purpose. The second set is for validation, which is responsible for simultaneously continuing to adapt the hyperparameters of the model, while providing an unbiased evaluation of the model fit. In addition to that, it can be used for regularization, as will be explored in a later section. Finally, the test set is an independent dataset of the two previous ones, which has the main objective to evaluate, in an unbiased way, the performance of the model in unseen data (data that was not used neither in the training process, nor in the validation process). 

One way to use the available dataset to evaluate the proposed models would be to simply apply the standard training-validation-testing division to the entire dataset available, but this strategy would be a poor way to use the small amount of data available. Typically, to evaluate machine learning models on a very limited time interval, k-fold cross-validation is used. This methodology consists, in the first place, in splitting the dataset into K folds. The higher the K value, the less biased the model is, but on the other hand, small folds (large value for K) can lead to errors due to lack of data, so one should choose the K value based on the data available for the specific application. Secondly, the model is trained in K-1 folds and validated on the remaining fold. This process is repeated until all the K folds have served as the validation set. The average performance on all validation folds shows a more robust result of the model performance evaluation than the standard train-validation-test methodology presented above. However, k-fold cross-validation can present some problems when applied to time-series problems, since one cannot choose random samples and assign them to either the validation set or the training set because it makes no sense to use future values to predict past values. There is a time dependency between observations, and it is essential to preserve this relationship during the entire process. To overcome this barrier, a nested cross-validation method was used. This validation methodology is an alternative to k-fold cross-validation that retains the temporal dependence of data. This procedure is quite complex, but according to the literature, it has the ability to "provide an almost unbiased estimate of the true error" \cite{nested}. It consists, in the first place, of splitting the data into different blocks, whose size progressively increases, implementing an expanding window approach. Then, each of these blocks must be split into train, validation and test data, where the validation data, as in the standard methodology, is temporally subsequent to the training data. In this procedure there are two loops, the inner loop and the outer loop. The inner loop is done with the purpose of tuning the hyperparameters for each of the blocks. After defining the optimal hyperparameters for each block, the model is retrained with these hyperparameters and tested in the test block. The outer loop consists of repeating this process for the 4 available blocks. 

After applying this procedure to the different blocks available, an average of the validation errors and an average of test errors presented in the different blocks is then calculated, thus reaching a total validation and testing error value. This methodology brings great advantages when the access to data is limited. It allows the user to make a much more robust evaluation of the performance of each model considering multiple scenarios.

In Figure \ref{partition}, the reader may find four different sub-datasets, resulting from a division of the main datastet, defined in phase 6, of the section \ref{chap3:subsec:data_completion}. The idea behind this division is to use 4 subsequent blocks for nested cross-validation, in order to conduct hyperparameter tuning of the different architectures proposed. In a second stage, after tuning the hyperprameters in each of the four blocks, one moves on to the test phase, in which the model to be tested, already with the hyperparameters defined, is retrained and tested in the Test set.

\begin{figure}[h!]
    \centering
    \begin{center}
    \includegraphics[width=1\textwidth]{Images/hyptun_.png}
    \caption{Data partition diagram.}
    \label{partition}
    \end{center}
\end{figure}

The performance evaluation of the models is then given by the average of the performance in each of these blocks i.e., the test error of the models is given by

\begin{equation}
     Model\ test\ error =\frac {1}{k}\ \sum_{i=1}^k Error_{di},
\label{err_av}
\end{equation}

where $Model\ test\ error$ represents the final model error for all the test sets, $Error_{di}$ represents the error that the model presented while performing in dataset $i$, and $k$ represents the number of datasets used.
In this specific case, the test error for each model is given by

\begin{equation}
     Model\ test\ error =\frac {1}{4}\ (Error_{testb1} + Error_{testb2} + Error_{testb3} + Error_{testb4}).
\label{err_av}
\end{equation}

The great advantage of using this method is the attenuation of the influence of random errors. In other words, by performing an average of the errors in the four different sets, the robustness of the selection process is increased since different scenarios are considered.


%\subsection{Data shifting}\label{chap3:subsec:data_shifting}

%While designing a predictive model, the training process is critical so that the model can learn from the data supplied to it. In a time series problem, it would not make sense for the input data to be of the same instant as the output data. When one wants to determine a future value based on current conditions, one needs to train the model to use the input data of the current instant to predict the output data of a future instant. It is then necessary to apply a shift to the output data so that training the model builds a predictive logic, also known as converting the time series into a supervised learning series. If the shift was not applied, one would be training the model to, based on the current instant data, determine the output data of the current instant, and that is not what is intended. As it is already known, one of the main objectives of this thesis is to forecast the available power of the building in 5, 10 and 15 minutes. To do that, depending on the model selected for that purpose, it might be necessary to develop 3 different supervised learning series. In Figure \ref{shifting}, the reader can find a graphical representation of the performed procedure.


%\begin{figure}[h!]
%    \centering
%    \begin{center}
%    \includegraphics[width=1\textwidth]{Images/Data Shift.png}
%    \caption{Data shifting procedure.}
%    \label{shifting}
%    \end{center}
%\end{figure}

%In a first instance, one has the data set that corresponds to the features identified as the input features of the model. For the initial scenario, the features of a given instant $t_i$, have a direct match with a value of Available Power of the output variables for the same instant $t_i$.  Then the transformation process begins, where the output variables will be shifted 5, 10 and 15 time-steps. The data vector of Available Power is copied originating three identical vectors. In the first step, the first 5, 10 and 15 measurements of each Available Power vector are removed, originating the output vectors with the ranges $[t_0 + 5, t]$, $[t_0 + 10, t]$, $[t_0 + 15, t]$ respectively. At this time, each set of input features already presents a match of Available Power in 5, 10 and 15 minutes, but the output vectors present different dimensions as one would expect. In order to have output vectors with the same dimension, the size of the three vectors is limited to the size of the smallest, as well as the set of input features. In the last step there is then a set of input features with values in the range $[t_0, t-15]$ and the ranges $[t\_0 + 5, t-10]$, $[t_0 + 10, t-5]$ and $[t\_0 + 15, t]$ for the Available Power to 5, 10 and 15 minutes, all four vectors with the same dimension. 

%It should be noted that, depending on the chosen model, it may or may not be necessary to use the three series, this will depend exclusively on the architecture of the model used. In any case, it was thought relevant to have the three series, one in which the input features for instant t link to the output feature for $t+5$, the second in which the input features for instant t link to the output feature for $t+10$, and finally, a third series in which the input features for instant t link to the output feature for $t+15$. Another point to mention is the reduction in the dataset dimension. There is a loss of 15 time-steps of data in this whole process. Since the total dataset contains a total of 201542 time-steps, the elimination of these 15 time-steps corresponds to a loss of only 0.0074$\%$ of the information.

\subsection{Feature selection}\label{chap3:subsec:feature_selection}

Although a total of 63 different features have been made available for the development of this work (Appendix \ref{chapter:appendixE}, Table \ref{tab:available_variables}), it is necessary to select which features to use in the development of a \ac{ML} model. Besides the 63 features provided by \ac{EDP} and \ac{FCUL}, 7 other features were also selected, which are listed in Appendix \ref{chapter:appendixE}, Table \ref{tab:computed_variables}. These new features concern calendar metrics such as $hour$, $day\_of\_week$, $day\_of\_month$, $month$ and $holiday$ (a binary indicator that states whether the day in question is a national holiday or not), and also concern calculated metrics such as $AvailablePower$, whose calculation is detailed in section \ref{chap3:sec:variable_to_predict}, and the $TheorethicalValue$, which concerns the theoretical value of the power generated by the \ac{PV} panels, detailed in the section \ref{chap3:subsec:data_completion}. 

A high number of features implies a high complexity of the system architecture. Even though all these features contribute in some way to enrich the past information available, it is important to limit the number of features used. It is then necessary to perform a pre-selection of which features are ideal for the system to produce the best possible results. 

It is certain that for a good model performance, the amount of available data is an extremely relevant factor. A model that has access to more information, in general, will present a better performance than a model that has access to less information. However, it must be ensured that the data provided to the model is relevant in the context of the problem, and that it actually helps to improve its performance. So far, a set of 70 features has been accessed, but the individual contribution of each feature in the context of the problem is not the same. Using the 70 features would produce a heavy and inefficient model, as there may be small groups of features whose contribution to the model is the same. We are then faced with a problem of dimensionality reduction. From the 70 features, it is necessary to extract those that are relevant in the context of the problem, and that actually contribute to a more capable model.

The concept of \ac{PCA} \cite{pca}, is then introduced. \ac{PCA} is a statistical procedure whose goal is to reduce the dimensionality of large datasets, replacing a high number of features by fewer features that still contain the relevant information from the original dataset. The dimensionality reduction can often incur in an increase of the error produced by the model, but the solution relies in giving up some minor accuracy for simplicity.


The \ac{PCA} is characterized by identifying the dimensions over which data are most dispersed. This way, the dimensions that best differentiate the data set under analysis are identified, that is, its main components. Since the \ac{PCA} is sensitive to the scale of the original data, the first step is to normalize the data. To do this, on the original data, the following formula was applied to every feature individually

\begin{equation}
   Z_i = \frac{x_i-\overline{x_i}}{\sigma_i},
   \label{PCA1}
\end{equation}

where $Z_i$ represents the normalized variable, $x_i$ the original variable, $\overline{x_i}$ the mean, and $\sigma_i$ the standard deviation. From the standardized variables, the correlation matrix $\rho$ is then calculated, where each position is given by


\begin{equation}
   Q_{ij} = \frac{1}{n-1}\sum_{i=1}^nZ_{i,n}Z_{j,n}.
   \label{PCA2}
\end{equation}

After obtaining the complete correlation matrix, $\rho$, one should then extract the eigenvectors and eigenvalues, two important factors that contribute for finding the directions of maximum variance and the the value of that variance, respectively. In fact, the eigenvalues are the variance of principal components. In order to extract eigenvectors and eigenvalues, one applied \ac{SVD}, that states that a matrix A with $n$ rows and $p$ can be decomposed in 

\begin{equation}
   A_{n,p} = U_{n,n} S_{n,p} V^T{p,p},
   \label{PCA3}
\end{equation}

where the columns of matrix $U$ are the left singular vectors, the $S$ is a diagonal matrix that has the same dimensions as A, and contains the singular values along the diagonal, and $V$ is the transposed orthogonal matrix and its rows are the right singular vectors. By multiplying $A^TA$, one gets exactly the matrix $\rho$, so 

\begin{equation}
   \rho = A^TA = VS^2V^T,
   \label{PCA4}
\end{equation}

thus V can be determined. Eigenvectors of matrix A are then found, and the $k$ eigenvectors that correspond to the $k$ directions of maximum variance are selected. The value of $k$ is chosen based on the cumulative explained variance, where the variance of several components is added together and divided by the total variance. This procedure is progressive, where the number of used components increases. With the increase of the number of components, there is also an increase in the cumulative explained variance until it stops increasing. The point where this parameter stops increasing and stabilizes is the ideal point, where one can find the minimum value of $k$ for which the cumulative explained variance is maximum. Later, a matrix containing the selected eigenvalues is constructed, which is later multiplied by the original matrix A. This results in a $n,k$ matrix, resulting from the entire dimensional reduction process, with the $k$ most relevant features in the context of the problem.

From the initial 70, a set of 13 features was considered useful in predicting the available power in the building. In Figure \ref{corr}, one can then observe the correlation matrix, where the correlations between all the 13 selected features are represented. The value of the correlation is present in the [-1, 1] range, where -1 means that the features present a perfect negative correlation, 0 means that there is no correlation between them, and 1 means that there is perfect positive correlation between the two features. 


\begin{figure}[h!]
    \centering
    \begin{center}
    \includegraphics[width=1\textwidth]{Images/corr1.PNG}
    \caption{Feature correlation matrix.}
    \label{corr}
    \end{center}
\end{figure}


The aim of computing this matrix is to identify features that present a perfect or almost perfect correlation between them, both positively and negatively. A perfect correlation between two features means that one can be deduced from the other, that is, the relevance of the two features for the construction of the forecasting system is the same, so one of the features can be eliminated. Using features that present a perfect correlation, one can incur in multicollinearity \cite{multicollinearity}. As can be seen in the matrix present in Figure \ref{corr}, none of the features selected by \ac{PCA} presents a perfect correlation with any other. Thus, there is a minimum set of features that offers a good representation of the available data. Although all these features represent the most significant set of variables, it is not guaranteed that using all 13 features is the combination that will produce the best results, but rather that it is the set of features that presents the most distinct information. It was then vital to understand which of the features selected would yield the best results. After a trial and error process where multiple combinations of features were tested, one verified that within the 13 features previously defined, there was a set of 8 features that would produce optimal results, being them: AVG\_GHI, RH\_avg, p\_amb\_avg, hour, month, holiday, day\_of\_week and AvailablePower. 
In summary, one started with a set of 70 features where the process of \ac{PCA} was applied reducing the set to 13. Then, through a trial and error procedure where several features were combined, a final set of 8 features was obtained, which will be used as input for the proposed models.



%After evaluating the projected correlation matrix, it was decided to start to eliminate the features that present an almost perfect correlation, both negative and positive. Starting with positive correlation, S represents the complex power consumed and P the real power consumed, which means that the two values only differ in the imaginary component. Since S does not have an imaginary component, S and P have the same value. The feature S was then discarded. The Theoretical Value and ActPwr also present a perfect correlation. This happens because, as explained in section \ref{chap3:subsec:data_completion}, the readings of the power generated by \ac{PV} panels (ActPwr) showed a lot of flaws. For a time failure longer than 30 minutes, the ActPwr value (which was null) was replaced by the calculated theoretical value of power generated by \ac{PV} panels. The large correlation is then justified by the fact that there were quite a few time flaws, i.e., most of the ActPwr values are similar to the Theoretical Value. The feature Theoretical Value was also discarded. 



%Also, although Theoretical Value and Avg\_GHI present a very high correlation with ActPwr (which is predictable given that the measured ActPwr these two variables were added, as explained in section \ref{chap4:vtp}), we chose to keep these two variables as input features since the correlation is not exactly perfect and can be decisive in certain cases in building a good predictive model. Therefore, the variables chosen for the input of all tested models are: Std\_DHI, Avg\_DHI, Avg\_GHI, Avg\_DNI, Avg\_POA, T\_amb\_avg, Theoretical\ Value, ActPwr, P, hour, day\_of\_month, day\_of\_week, month and holiday. In addition, the variable Available Power given in W has been added as an input feature, resulting in a total of 15 input features and 3 output (Available Power in 5, 10 and 15 minutes).

\section{Proposed model}\label{chap3:sec:proposed_model}

In Chapter \ref{chap:background}, some of the most common models were studied with regard to forecasting power consumption in various types of buildings. As has also been mentioned, data-driven models have been widely used in this type of applications. In this thesis, it was also decided to study a solution composed by one or more data-driven models, since historically this class of model have a better performance and an excellent relation between the need of computational resources, and the obtained performance.

This problem presents some characteristics that are determinant in the process of choosing the models to be tested. First of all, the data made available for this study, represents a significantly short time period, which means that the use of models, as statistical regressions, are not ideal to solve this problem. 

On the other hand, the challenge presented by \ac{EDP} implies a 5, 10 and 15 minute data forecast. This factor implies the use of a fast model that is able to produce different forecasts in a short period of time. This is a problem for \ac{SVM}s/\ac{SVR}s because, as mentioned by Zhao et al.\cite{svm3}, Dong et al.\cite{svm2} and Li et al.\cite{svm5}, while producing results with good precision an accuracy, they are slow models for large-scale problems. 

Another model presented in Chapter \ref{chap:background}, were the \ac{DT}s. Although they are simple to apply models, \ac{DT}s have the limitation of not allowing a future value to be computed, but rather a range of values in which the variable to be predicted can be found. Since for this problem the main objective is to obtain specific values of available power, this category of models was not considered in this research.

Although all the mentioned models can produce acceptable results for the proposed work, it was decided to apply different types of artificial neural networks, both architectures with only one \ac{ANN} layer, and architectures with multiple layers implemented simultaneously. The idea is then to test various typologies of \ac{ANN}s, and elaborate a comparative study between the proposed solutions, comparing their performance in different scenarios and conditions.

\subsection{Artificial Neural Networks}\label{chap3:subsec:artificial_neural_networks}

Neural networks have proven to be 
extremely efficient in identifying and learning temporal data patterns, many of them quite complex. In chapter \ref{chap:background}, two large families of neural networks were presented, \ac{FFNN}s and \ac{RNN}s. From the different existing architectures within these two categories, two types of layers were selected, \ac{GRU} and \ac{LSTM}, both classified as \ac{RNN}s' layers. In addition, \ac{1D CNN} layers were also tested, which fall under the umbrella of \ac{CNN}. These three different types of layers were tested in multiple arrangements and combined with each other to form multiple neural network architectures.

\subsubsection{Long Short-Term Memory (LSTM)}\label{chap3:subsubsec:lstm}

The first type of configuration considered is the \ac{LSTM}, architecture that fits within the group of \ac{RNN}s, proposed by Hochreiter and Schmidhuber et al. \cite{lstm0} in 1997. \ac{LSTM}s were invented to fight the vanishing gradient problem that is often identified during the training process of common \ac{ANN}s with gradient-based learning methods and back propagation. In these cases, each of the \ac{ANN}'s weights receives an update proportional to the partial derivative of the error function from the current weight in each iteration of training. The problem is that, in some cases, the gradient becomes smaller and smaller with each iteration, preventing the weight from changing its value. This problem leads to a poor performance of the \ac{ANN}, and can even prevent it from continuing the training process. To avoid this situation, the \ac{LSTM} units are composed of a cell state, an input gate, an output gate and a forget gate, as can be seen in Figure \ref{lstm}.

\begin{figure}[h!]
    \centering
    \begin{center}
    \includegraphics[width=0.6\textwidth]{Images/LSTM_cell_detailed.png}
    \caption{LSTM unit.}
    \label{lstm}
    \end{center}
\end{figure}

The responsibility of the cell state is to trace the dependencies between the elements in the input sequence. The forget gate has the purpose of forgetting information that is no longer useful to the state of the cell. This gate has two inputs, $x_t$ (input at the specific time) and $h_{t-1}$ (output from previous cell) which are multiplied by weight matrices, followed by the addition of the \textit{bias}. The result of this process is passed through a sigmoid $\sigma$ activation function, which provides a binary output. If for a given cell state the output is 0, the information is forgotten, if the output is 1, the information is retained for future use.

The input gate is responsible for adding useful information to the cell state. First, the information is regulated using the sigmoid function that filters the values to be remembered in a similar way to the forget gate, also having as inputs $h_{t-1}$ and $x_t$. In parallel, a vector is created using the $tanh$ function that has as output a value in the range $ [-1, +1]$, which contains all possible values of $h_{t-1}$ and $x_t$. The values of the vector and the regulated values are then multiplied. 

The output gate's task is to extract useful information from the current cell state to be presented as an output. First, a vector is generated by applying the function $tanh$ to the cell. In parallel, the information from the $h_{t-1}$ and $x_t$ inputs is regulated using the sigmoid function that filters the values to be remembered. The values of the vector and the regulated values are then multiplied to be sent as an output and input to the next cell.

 


The equations that describe the operation of a \ac{LSTM} unit are given by 

\begin{equation}
    \begin{cases} 
        
        f_t=\sigma(W_f.[h_{t-1},x_t] + b_f)\\
        i_t=\sigma(W_i.[h_{t-1},x_t] + b_i)\\
        \widetilde{C}_t = tanh(W_c.[h_{t-1},x_t] + b_C)\\
        C_t=f_t*C_{t-1}+i_t* \widetilde{C}_t\\
        o_t=\sigma(W_o.[h_{t-1},x_t] + b_o)\\
        h_t=ot*tanh(C_t)
        
         
    \end{cases} ,
\end{equation}

where $f_t$ represents the forget, $i_t$ the input gate, $\widetilde{C}_t$ the cell input activation vector, ${C}_t$ the cell state vector, $o_t$ the output gate, $h_t$ the hidden state vector or output vector of the \ac{LSTM} unit. This type of units has then the ability to choose new information that is relevant and delete the past information that is not relevant, maintaining a good functioning, particularly in time series data since there can be lags of undefined length between important events in a time series dataset.

\subsubsection{Gated Recurrent Unit (GRU)}\label{chap3:subsubsec:gru}

The \ac{GRU} unit is quite similar to \ac{LSTM} with the particular difference that it has no output gate. Introduced in 2014 by Kyunghyun Cho et el. \cite{gru0}, \ac{GRU}s are also part of the \ac{RNN} family. Although they have a similar architecture to \ac{LSTM}s, \ac{GRU}s are simpler models which led to the conclusion that for some cases, where there are smaller datasets and with a lower frequency of repetition, \ac{GRU} architectures perform better than \ac{LSTM} architectures \cite{gru1}, \cite{gru2}.

Compared to \ac{LSTM}s, \ac{GRU}s have no cell state and use the hidden state to transfer information. This architecture has only two gates, a reset gate and an update date, as can be seen in the diagram in Figure \ref{gru}.


\begin{figure}[h!]
    \centering
    \begin{center}
    \includegraphics[width=0.6\textwidth]{Images/GRU_cell_detailed.png}
    \caption{GRU unit.}
    \label{gru}
    \end{center}
\end{figure}


The \ac{GRU} structure allows to capture dependencies of large data sequences without discarding information from previous parts of the sequence. This is achieved through its gates, similar to \ac{LSTM}s. These gates are responsible for regulating the information to be kept or discarded at each time interval.

The \ac{GRU}'s ability to maintain a long term memory stems from calculations in the \ac{GRU} unit to produce the hidden state. While \ac{LSTM}s have two different states passed between cells - the cell state, which carries long-term memory, and the hidden state, which carries short-term memory - \ac{GRU}s have only one hidden state transferred between time stages. This hidden state is able to maintain both long term and short term dependencies simultaneously, due to the constraint mechanisms and calculations through which the hidden state and the input data pass.  Like the \ac{LSTM} gates, the gates in the \ac{GRU} are trained to selectively filter out any irrelevant information, keeping what is useful.

These gates essentially produce vectors containing the values 0 or 1 that will be multiplied with the input data and/or hidden state. A value of 0 in the vectors indicates that the corresponding data in the input or hidden state is not important and therefore will be overridden. On the other hand, a value of 1 in the array means that the corresponding data is important and will be used.


The equations that describe the operation of a \ac{GRU} unit are given by 

\begin{equation}
    \begin{cases} 
        
        z_t = \sigma(W^{(z)} x_t + U^{(z)} h_{t-1} + b^{(z)})\\
        r_t = \sigma(W^{(r)} x_t + U^{(r)} h_{t-1} + b^{(r)})\\
        \tilde{h}_t = \tanh(W^{(h)} x_t + U^{(h)} h_{t-1} \circ r_t + b^{(h)})\\
        h_t = (1-z_t) \circ h_{t-1} + z_t \circ \tilde{h}_t

    \end{cases} ,
\end{equation}

where $zt$ represents the update gate vector, $r_t$ the reset gate vector, $\tilde{h}_t$ the candidate activation vector and $h_t$ the output vector. \ac{GRU}s have fewer tensor operations to perform, and so they take less time to train than \ac{LSTM}s.  

\subsubsection{One Dimensional Convolutional Neural Network (1D CNN)}\label{chap3:subsubsec:1dcnn}


Convolutional Neural Networks (\ac{CNN}s) are biologically-inspired feed-forward neural networks that have been successfully applied in the field of computer vision and digital image processing and analysis \cite{cnn0}.

In \ac{ML} applications, \ac{2D CNN} are typically used for categorizing 2D images (into classification problems) or creating images (in case of regression-based problems). In this kind of applications, the layers extract two-dimensional patches from an image and then apply similar transformations to each of the patches. Similarly, there are \ac{1D CNN}, that extract data subsequences from a dimension of the main sequence. This thesis aims to predict the future behavior of certain variables of a single dimension, therefore models consisting of \ac{1D CNN}'s layers have been tested.

The \ac{1D CNN} is very effective when you expect to derive interesting features from shorter (fixed-length) segments of the overall data set. Its main purpose is to turn the long input sequence into much shorter (downsampled) sequences of higher-level features, which can be very useful in the treatment of large time sequences.

In Figure \ref{conv1}, the reader may find a representation of the normal working process of a \ac{1D CNN}. The operation of \ac{1D CNN} is based on the extraction of odd size patches (in this particular case it uses a convolution window with size 5), from a sequence of large dimensions of input data. Is is then applied a convolution operation that can be written as \cite{cnn2} 

\begin{equation}
y(n)=
    \begin{cases} 
            
        \sum_{i=-\frac{k}{2}+\frac{1}{2}}^{\frac{k}{2}-\frac{1}{2}} x(n+i)h(i),\  if \  n=0.\\
        \sum_{i=-\frac{k}{2}+\frac{1}{2}}^{\frac{k}{2}-\frac{1}{2}} x(n+i+(s-1))h(i),\  otherwise.\\
    
    \end{cases} ,
\end{equation}

where $n$ represents the length of the convolution layer, $x$ represents the input of the layer, $h$ represents the kernel (or extracted patch) and $k$ its length. Lastly, $s$ represents the number of shifted positions of the kernel window after each convolution, also known as number of strides. For example, if n = 20, k=5 and s=1 then

\begin{equation}
    \begin{cases} 
        y(2)=x(0)h(0)+x(1)h(1)+x(2)h(2)+x(3)h(3)+x(4)h(4)\\
        y(3)=x(1)h(0)+x(2)h(1)+x(3)h(2)+x(4)h(3)+x(5)h(4)\\
        \  ...\\
        y(17)=x(15)h(0)+x(16)h(1)+x(17)h(2)+x(18)h(3)+x(19)h(4)\\
    \end{cases} .
    \label{noncausal}
\end{equation}

It should be mentioned that the output ($o=16$) does not have the same dimension as the input layer ($n=20$). This happens because no padding has been applied to the convolution which implies that for the length of the convolution layer $n$, the kernel size $k$, and the number of strides $s$, the output $o$ is given by:

\begin{equation}
    o = \lfloor \frac{(n-k)}{s} \rfloor + 1 .
\end{equation}

By applying "padding", having the input $n$, kernel size $k$, and padding with length $p$, the output $o$ is given by:

\begin{equation}
    o = \lfloor \frac{(n+2p-k)}{s} \rfloor + 1 .
\end{equation}

This allows to obtain \ac{1D CNN}'s layers whose input size is equal to the output. In this example, for the output $o$ dimension to have the same dimension as the input sequence, 20, then the $p$ dimension must be 2. In Figure \ref{conv1} it can be seen schematically the result of the described convolution operation.

\begin{figure}[h!]
    \centering
    \begin{center}
    \includegraphics[width=1\textwidth]{Images/conv1.png}
    \caption{1D convolution working process.}
    \label{conv1}
    \end{center}
\end{figure}

Although it may seem like a good methodology, the convolution operation presents a particularity: it uses future values in its computing procedure. In the example present in Figure \ref{conv1}, it can be observed that in the computation of $y(10)$, for example, values of $x(8)$, $x(9)$, $x(10)$, $x(11)$ and $x(12)$ were used. In a series with time dependencies such as the problem in question, the use of future values in the computation of a present value presents an obvious violation, in which the output of instant $[t]$ depends on the input of $[t+1:]$. In this sense, the concept of causal convolution must be introduced. 

In causal convolution, the value of the generated output is not dependent on future inputs. In the same way described above, the causal convolution is given by 

\begin{equation}
y(n)=
    \begin{cases} 
            
        \sum_{i=0}^k x(n-i)h(i),\  if \  n=k-1.\\
        \sum_{i=0}^k x(n-i+(s-1))h(i),\  otherwise.\\
    
    \end{cases} ,
\end{equation}

where $n$ represents the length of the convolution layer, $x$ represents the input of the layer, $h$ represents the kernel (or extracted patch) and $k$ its length. Lastly, $s$ represents the number of shifted positions of the kernel window after each convolution, also known as number of strides. For example, if n = 20, k=5 and s=1 then

\begin{equation}
    \begin{cases} 
        y(4)=x(4)h(0)+x(3)h(1)+x(2)h(2)+x(1)h(3)+x(0)h(4)\\
        y(5)=x(5)h(0)+x(4)h(1)+x(3)h(2)+x(2)h(3)+x(1)h(4)\\
        y(6)=x(6)h(0)+x(5)h(1)+x(4)h(2)+x(3)h(3)+x(2)h(4)\\
        \  ...\\
        y(19)=x(19)h(0)+x(18)h(1)+x(17)h(2)+x(16)h(3)+x(15)h(4)\\
    \end{cases} .
    \label{noncausal}
\end{equation}

In Figure \ref{conv2}, a schematization of the causal convolution process can be seen.

\begin{figure}[h!]
    \centering
    \begin{center}
    \includegraphics[width=1\textwidth]{Images/conv2.png}
    \caption{1D causal convolution working process.}
    \label{conv2}
    \end{center}
\end{figure}

In this layer category, convolutional windows are used, which is similar to using a k-dimensional kernel where all its elements have the value $\frac{1}{k}$. Following the example, with the kernel described in dimension 5, $y(19)$ is given by
\begin{equation}
        y(19)=x(19)\frac{1}{5}+x(18)\frac{1}{5}+x(17)\frac{1}{5}+x(16)\frac{1}{5}+x(15)\frac{1}{5}.
    \label{19}
\end{equation}



Usually, along with the \ac{CNN} layers, pooling layers are used. The pooling operation aims to simplify convolution output, giving more importance to prominent features and patterns. There are several types of pooling, the most important of which is Max pooling which involves selecting the largest value in a user-defined size window, thus reducing the dimensionality of the output. Following the example, for a $pool\_size$ of 5 units, one can see in Figure \ref{maxpooling}, the representation of the Max pooling operation where the dark units have the highest value within the 5-unit window. For the first 10 time-setps, the highest value value is then passed as layer output, and the dimension of the output decreases. In other words, for each set of 5 values, the largest is chosen, and the size of the output decreases with an inverse relationship to the chosen $pool\_size$. In this case, the dimensionality of the output decreases five times.

\begin{figure}[h!]
    \centering
    \begin{center}
    \includegraphics[width=0.9\textwidth]{Images/maxpooling.png}
    \caption{Max pooling without and with padding.}
    \label{maxpooling}
    \end{center}
\end{figure}

If the user intends to apply a normal pooling process, there is an obvious reduction in dimensionality. If one intends to extract the most relevant features while maintaining the same dimensionality, it is also possible. To accomplish this, padding should also be added to this layer. In Figure \ref{maxpooling}, for the last 10 time-steps, it is possible to observe Max pooling with padding, where the beginning of the process is similar to pooling without padding, but inside the output windows of dimension $pool\_size$, the higher selected value is copied to all the remaining empty spaces. 

In order to aggregate all the information entered regarding the layers {1D CNN}, a simple example is shown in Figure \ref{resdata}, which illustrates the process of convolution between a simple size 5 kernel and a random time signal.

\begin{figure}[h!]
\captionsetup[subfigure]{position=b}
\centering
\subcaptionbox{\label{inputseq}}{\includegraphics[width=.32\linewidth]{Images/inputsequence.png}}
\subcaptionbox{ \label{afterconv}}{\includegraphics[width=.32\linewidth]{Images/afterconvolution.png}}
\subcaptionbox{ \label{aftermaxp}}{\includegraphics[width=.32\linewidth]{Images/aftermaxpooling.png}}
\caption{Example data a) Before convolution b) after convolution c) after Max pooling.}
\label{resdata}
\end{figure}


Figure \ref{inputseq} shows a 20 time-steps random signal, as the one referred in the other examples used in this section. At this stage, the signal is in its original form, without any treatment. Figure \ref{afterconv} represents the result after the convolution operation with kernel\_size 5. It can be seen that at this point, the signal is a smoothed representation of the initial signal. The convolution operation extracts the main characteristics of the signal, ignoring intermediate oscillations. Afterwards, the Max pooling operation with pool\_size 5 is applied to the signal resulting from the convolution operation. The result of this operation is represented in Figure \ref{aftermaxp}. In red, is the result of the operation of Max pooling without padding, where the main points of the sequence are extracted, thus reducing the dimensionality of the time series. In grey, the same operation can be observed but with padding,  which consists of extending the point with the highest value within each of the 4 defined windows. This is the final signal resulting from the convolution operation with Max pooling, a simplified representation of the initial sequence. It is important to point out that the process described is a very simplified representation of the way the layers \ac{1D CNN} work. In practice, this process is much more complex, and is carried out taking into account multiple variables simultaneously. A more detailed description of this process will be provided later on. 

\subsection{Networks specifications}\label{chap3:subsec:networks_specifications}

The process of designing the architectures to be tested is a complex and demanding task. This section describes the architectures used, as well as some techniques used in their development.

\subsubsection{Studied architectures}\label{chap3:subsubsec:studied_architectures}
In the development of this thesis, different types of neural networks were tested, composed by the layers introduced in section \ref{chap3:subsec:artificial_neural_networks}. The objective was to study the influence that each of the three layers introduced has on a predictive model, and to study the results obtained by pairing multiple layers into a single model. 

\paragraph{Vanilla architecture}

The first architecture proposed for this study is a simple architecture, based on the many-to-one concept, which consists of using the input features of the last instants to predict a single value in the future. 
In Figure \ref{arc1} one can see an illustrative scheme of how this architecture works.

\begin{figure}[h!]
    \centering
    \begin{center}
    \includegraphics[width=0.7\textwidth]{Images/arc1.png}
    \caption{Vanilla architecture}
    \label{arc1}
    \end{center}
\end{figure}

This is a very common architecture that uses the capacity of the \ac{RNN}s to predict a single future value of the sequence. In Figure \ref{arc1}, one can see the working mode of the architecture where on the left side there is a representation of the functioning of a RNN cell, and on the right side there is a more detailed representation of the functioning of that same cell. As explained in section \ref{chap3:subsec:artificial_neural_networks}, there are two types of cells \ac{RNN} that present this type of behavior, the cells \ac{GRU} and \ac{LSTM}. In this sense, two models have been created, Model 1.1 whose architecture is similar to the structure presented in Figure \ref{arc1} and uses cells of type \ac{GRU}, and Model 1.2 whose architecture is similar but uses cells \ac{LSTM}. A relevant point to mention is that, as can be seen in Figure \ref{arc1}, for each $xt$ a $yt$ is generated. In this case, it is intended to forecast three values with a many to one architecture. The problem was modeled so that $yt$ corresponds to a vector $yt = [y(t+5), y(t+10), y(t+15)]$, that is to say that the Vanilla architecture was adapted to produce an output with three values at each iteration. To make this possible, it was necessary to convert the time series so that, based on the last $N$ features readings, it could predict a vector composed of three values, a value corresponding to the available power in (t+5), one corresponding to the available power in (t+10) and finally, one with available power (t+15). This process is also known as converting the time series into a supervised learning sequence.



\paragraph{Encoder-Decoder architecture}

The second proposed architecture is known as Encoder-Decoder.  This is a many-to-many architecture, that is, from a past data sequence, the architecture is able to predict a new data sequence. Contrary to the previous architecture, which produced only one value per iteration, this architecture uses the values up to instant $t$ to produce data sequences for the next $N$ steps $(t+1, t+2, ..., t+N)$. This architecture is then more suitable in the context of this problem, since it can be used to predict for each instant $t$, the next 15 steps $(t+1, t+2, ..., t+15)$. In Figure \ref{arc3}, a schematic representation of the functioning mode of the Encoder-Decoder architecture can be observed. 

\begin{figure}[h!]
    \centering
    \begin{center}
    \includegraphics[width=0.9\textwidth]{Images/arc2.png}
    \caption{Encoder-Decoder architecture}
    \label{arc2}
    \end{center}
\end{figure}

As can be seen in Figure \ref{arc2}, this architecture is subdivided into two levels. The encoder level, present on the left, and the Decoder level, present on the right. The encoder is composed of a set of recurrent cells, responsible for aggregating important information about the input in the Encoder Vector. The decoder, present on the right side of the Figure \ref{arc2}, is responsible for using the information present in the encoder vector to make a forecast for the next $N$ steps. It can also be seen that the output of the first step of the decoder is entered as input for the production of the next value of the sequence, and so on. This is a fundamental feature that enhances the production of the sequence. As in the Vanilla architecture, the Encoder-Decoder architecture has been implemented with two types of cells, \ac{GRU} which refers to Model 2.1 and \ac{LSTM} which refers to Model 2.2. Since this one is a many-to-many architecture, again supervised learning sequences had to be developed\footnote{Both the code of this step as well as the rest of the developed code can be found at \cite{code}} that took into account both the size of the input sequence and the output sequence. In other words, unlike the Vanilla architecture, based on the last $N$ features readings, the model must be able to predict a set of fifteen available potency values, a value corresponding to the available potency in (t+1), one corresponding to the available potency in (t+2) and so on up to (t+15). In the Vanilla case a vector with only three values is produced, and in the Encoder-Decoder case fifteen different values are produced (no vector).




\paragraph{1D CNN Encoder-Decoder architecture}


The third architecture proposed is similar to the Encoder-Decoder architecture, but in this case the Encoder, instead of being formed by a \ac{RNN} similar to the Decoder, is formed by a \ac{1D CNN}. Using the \ac{1D CNN} as an encoder may bring some advantages, namely the ability of this type of layers to identify small patterns in the input data. As name indicates, the \ac{1D CNN} layer applies the process of convolution between two functions, determining the relationship between them. The integral of this convolution, in turn, expresses how the form of a function is altered by the form of another function. This type of network is usually used in image processing, but by applying it to signals of only one dimension, as is the case with the input features available for the model to be developed, it is possible through the convolution process to identify how small patterns in one feature can affect the others. For this reason, it is then possible to combine this capacity if \ac{1D CNN}s with the predictive capacity of \ac{RNN}s from a hybrid model Encoder-Decoder. In Figure \ref{arc3}, the reader can find a diagram illustrating the operation mode of the Hybrid Encoder-Decoder model.

\begin{figure}[h!]
    \centering
    \begin{center}
    \includegraphics[width=1\textwidth]{Images/ED.png}
    \caption{1D CNN Encoder-Decoder architecture}
    \label{arc3}
    \end{center}
\end{figure}

By examining Figure \ref{arc3}, one can see how the third architecture proposed works. Given a temporal signal, where the length represents the number of timesteps and the width the number of variables in a multivariate temporal series, the convolution kernels, with the same width as the temporal series, are sliding windows of defined length that travel through the temporal series performing the convolution process. The kernel elements are multiplied by the elements of the time series that they pass through at each instant. Then, the results of the multiplications are added together and a non-linear activation function is applied to the resulting value. As a result of the whole process, an element of an univariate time series is obtained. Then the kernel moves down one position and this process is repeated, generating the second value of the univariate time series. This is the process performed by a kernel. The number of univariate time series obtained corresponds to the number of kernels used. A set of univariate time series is then obtained which can be said to be "filtered". The second stage involves carrying out the Pooling process with each time series where the largest values are selected. A new vector, of smaller dimension, is then formed for each series. Then, all the vectors are flattened generating a single vector of larger dimension. This vector corresponds to the encoder vector detailed in Architecture 2. From this point on, the process is similar to the previous architecture. 

As in the other architectures, this architecture can be divided into two models, Model 3.1 where the recurring cells are \ac{GRU}s, and Model 3.2 where the reoccurring cells are \ac{LSTM}s. 



%In Figure \ref{models}, are represented the five main architectures studied.

%\begin{figure}[h!]
%    \centering
%    \begin{center}
%    \includegraphics[width=1\textwidth]{Images/models.png}
%    \caption{Tested model architectures.}
%    \label{models}
%    \end{center}
%\end{figure}

%Model 1 and Model 2 are composed of a single layer of \ac{GRU} and \ac{LSTM} respectively. The purpose of these two models is to compare the performance between the two types of single layer. These two models were chosen, because both architectures are already known to perform well when it comes to time series forecasting problems. So one has a set of two simple models with predictive capabilities. These two models use as input all the variables selected through the \ac{ac} process based on the assumption that all the features help to predict the target, the Available Power of the building in the feature. 

%Although the \ac{PCA} procedure detailed in section \ref{chap3:subsec:feature_selection}, which defined the best features to be used as inputs of the selected models, it was decided to test the models with a single input feature, the past Available Power. The reason behind this choice is that it is suspected that a more complex model that has access to a larger number of features can also be easily deceived by those features. Once there is access to past measurements of the variable to predict, it was also experimented to use only this variable as an input feature. Models 3 and 4 present an architecture quite similar to the architecture of models 1 and 2, with the slight difference that only the past records of the output variable are used as input, i.e., with input only the measure of available power is given, and as output is expected the future value of available power and 5, 10 and 15 minutes. The study of these two models aims to evaluate simpler models with access to fewer variables, and compare their performance with models 1 and 2 that use all the features they have available to meet all the necessary requirements. Although the best 13 input variables from a set of 70 have been defined, nothing guarantees that the use of the larger set of variables will produce better results than the introduction of only past values of the variable to be predicted. 

%The layers \ac{1D CNN} layers introduced in section \ref{chap3:subsubsec:1dcnn}, thanks to their mode of operation, are often used for feature extraction on input data, since they have the ability to identify patterns in the input features. By combining this capability with the capability of \ac{GRU} and \ac{LSTM}s to support sequence prediction, it is hoped to obtain more capable models for predicting the proposed variable. In order to study the effect that the combination of \ac{CNN}s with \ac{GRU}s and\ac{LSTM}s has, the architecture of model 5 is similar to that of model 3, with the addition of a set of \ac{1D CNN} layers, and a Maxpooling layer before the \ac{GRU} layer. Similarly, model 6 is like to model 4 with the addition of a set of \ac{1D CNN} layers and a Maxpooling layer before the \ac{LSTM} layer. 

%All six architectures present, as their last layer, a Dense layer with three units, responsible for converting the outputs of the previous layers into only three outputs, one for each of the forecasts ((t+5), (t+10) and (t+15)). There are a total of two multivariate models, which use all the variables selected as input features, two similar models but using only one input feature, and two hybrid models with combine the capabilities of convolutional networks with the predictive capabilities of the models proposed so far. 


\subsubsection{Data standardization}\label{chap3:subsubsec:data_standardization}

As mentioned in section \ref{chap3:subsec:feature_selection}, there are several features that can be used in the implementation of \ac{ML} methodologies, namely in the use of \ac{ANNs}. Besides the several features have different scales, the \ac{ANNs} have a better performance when the data is standardized. Also, if inputs are not limited to a range like (0.1) or (-1.1) the model used will have difficulty in equally distribute importance of each feature, thus naturally large values become dominant relatively smaller values during the training of \ac{ANNs}.

In order to avoid this kind of failures, a $MinMaxScaler$ \ref{minmax} was applied, whose mode of operation consists in transforming each feature individually, scaling it to a given interval(by default (0.1)).  Arithmetically, this function is given by

\begin{equation}
     X_i' = \frac{X_i - min(x) }{max(x)-min(x)},
\label{minmax}
\end{equation}

where $X_i'$ represents the rescaled value, $X_i$ the original value, $min(x)$ is the minimum value in the specific feature, and $max(x)$ is the maximum value in the feature. The standardized feature is then obtained in the defined range. Usually, this process consists of two phases, a phase in which the scaler used is adapted to the data and a phase in which it is used to transform the data. It is good practice to use the training data to adapt the scaler and transform the data, and the test data is just simply transformed. This reduces the \textit{bias} associated with transforming unknown data. The inverse process to that presented in the equation \ac{ANN} is applied to the output of the \ac{ANN}, in order to transform the data back to its original scale.



\subsubsection{Hyperparameter optimization} \label{chap3:subsubsec:hyperparameter_optimization}

A hyperparameter is a parameter whose value is used to control the learning process. Hyperparameter optimization is the process of choosing a set of optimal hyperparameters for a learning algorithm. Depending on the layer in question, different hyperparameters can be tuned. Other parameters, such as node weights are initialized randomly, and are defined during the learning process. In other words, hyperparameters are parameters which must be changed by the user according to the results obtained, while other parameters should not be modified by the user, and are the result of the learning process.

\ac{ANN}s have numerous advantages, both in terms of their usage and performance. On the other hand, tuning the hyperparameters is an extremely time-consuming process that is done by hand. A certain set of hyperparameters is trained, validated, and then changed and so on until the best possible result is obtained. During the validation phase, different hyperparameters were tuned, such as the number of nodes in each layer, number of filters of each \ac{1D CNN} layer and kernel size. Exhaustive grid search, a brute-force technique, was then used, which consists in testing all possible combinations of a hyperparameter user-defined grid. For each combination and hyperparameters, the models were evaluated using the validation methodology introduced in section \ref{chap3:subsec:data_partition}. This way, it was possible to obtain the best combination of hyperparameters for the available data set.

\subsubsection{Regularization techniques}\label{chap3:subsubsec:regularization_techniques}

During the learning process of an \ac{ANN}, it is possible that overfitting occurs. Overfitting is a term used to describe when a model fits very well to the previously observed data set, but proves to be ineffective in predicting new results. In practice, overfitting causes the model to perform very well during training, but the performance gets much worse when faced with brand new data because the model has adapted too much to the training data.  

Regularization refers to a set of different techniques that lower the complexity of a neural network model during training, and thus prevent the overfitting \cite{reg0}. There are several types of regularization techniques. In this thesis, two techniques were used: Early-stopping and Dropout.

\paragraph{Early-stopping}\label{sec:early}

Early-stopping is probably the most common regularization technique. In Figure \ref{early} the reader can find a two graphs that portray the evolution of the error over epochs of a prediction model. During the training and validation process of the predictive models, the error associated with the training set tends to decrease with the increase of the number of training epochs. In an optimal case, with the increase of the number of validation epochs, the error associated with the training set also tends to decrease, thus reaching a trained and validated model whose error produced at validation is minimal. In the case of datasets large enough to overfit, it is observed that most of the time the training set error also tends to decrease, but the validation error starts to increase at a certain point. This is one of the main ways to identify overfitting.

\begin{figure}[h!]
\captionsetup[subfigure]{position=b}
\centering
\subcaptionbox{\label{early0}}{\includegraphics[width=.45\linewidth]{Images/early-stopping0.png}}
\hspace{0.05\textwidth}
\subcaptionbox{ \label{early1}}{\includegraphics[width=.45\linewidth]{Images/early-stopping1.png}}
\caption{Prediction error evolution over epochs a) without overfitting b) without overfitting.}
\label{early}
\end{figure}

In Figure \ref{early0}, an ideal case is represented where there is no overfitting. It can be verified that the evolution of the validation error follows the evolution of the training error. Figure \ref{early1} presents a clear case of overfitting in which, at the moment marked with the black dot, the model presents its smallest validation error, which later increases. The early-stopping process consists in: from the moment the system detects that the validation error increases, it runs for more $p$ (patience factor) epochs where it gives the system the opportunity to obtain lower values of validation error. If in none of the $p$ epochs a lower value is found, the training process ends, eraly-stopping occurs. In the extreme case where $p$ = 0, the system stops the training process as soon as it detects a higher value than the previous one for the validation error, moment marked by the black circle present in Figure \ref{early1}.

\paragraph{Dropout}

In the process of training a fully connected \ac{ANN}, neurons tend to develop an interdependence between each other, which limits their individual capacity, leading to overfitting.


To tackle this problem, the dropout is used, which is also a widely used regularization technique. In Figure \ref{drop0} it is possible to see a \ac{ANN} without a dropout, and in Figure \ref{drop1} it is possible to see the same \ac{ANN} after dropout is applied.

\begin{figure}[h!]
\captionsetup[subfigure]{position=b}
\centering
\label{fig:drop}
\subcaptionbox{\label{drop0}}{\includegraphics[width=.4\linewidth]{Images/dropout0.png}}
\hspace{0.05\textwidth}
\subcaptionbox{ \label{drop1}}{\includegraphics[width=.4\linewidth]{Images/dropout1.png}}
\caption{Artificial neural network a) without dropout b) with dropout.}
\end{figure}

The application of dropouts, implies for each hidden layer, for each sample of training, and for each iteration, to ignore a fraction $p$ of random neurons (and corresponding activations) during the training process. During the validation phase, all activations are then used, but reduced by a $p$ factor to take into account the missing activations lost during the training \cite{drop0}.

\section{Implementation environment}\label{chap3:sec:implementation_enviromnet}

During the development of the thesis, several scripts were implemented, both for data processing and for the implementation of machine learning models. The language chosen for this purpose was \textit{Pyhton}. The primary reasons for this choice were the ease of syntax of this language, as well as the large number of libraries available, including \textit{keras}, a Deep Learning library that provides all the necessary tools for building and deploying Neural Networks.

Regarding the hardware components used, two different systems were used during the development of this work. The first environment consists of a CPU (Intel Core i5-3470 3.20GHz), and a GPU (NVIDIA GeForce GTX 1050 Ti) essential to accelerate the training process of the proposed models. The second environment consists of a virtual environment implemented on \textit{Microsoft Azure Machine Learning} platform, where a cluster consisting of a 6 core processor, and a GPU (NVIDIA Tesla K80).

The first setup was used with the purpose of testing, in a first phase, several models with simple tasks, and the second environment was used to put into practice more complex tests that required more computational capacity.
	
\section{Performance evaluation metrics}\label{chap3:sec:performance_evaluation_metrics}

In the previous section, important details concerning the data used in this thesis were explained. One defined $Model\ error$, but did not specify which concrete metrics were used in this work. To compare the performance of the different models in the defined dataset, it is necessary to use some measures of forecast performance. In this sense, during this thesis four different measures were used that relate the real values present in the time series represented by $y_t$, and the forecast values, which are represented by $f_t$. Thus, the forecast error is given by $e_t=y_t-f_t$. The three measures presented are quite common in the literature \cite{errors} and are described below.

\subsection{Mean Absolute Error (MAE)}

The \ac{MAE}, is defined as

\begin{equation}
     MAE =\frac {\sum_{t=1}^n|e_t|}{n}.
\label{mae}
\end{equation}

The \ac{MAE} is known as a scale-dependent accuracy measure and measures the average absolute deviation between the forecasted values and the real ones. As a scale-dependent measure, it cannot be used to compare series using different scales.


\subsection{Mean Squared Error (MSE) and Root Mean Squared Error (RMSE)}

The \ac{MSE}, is defined as

\begin{equation}
     MSE =\frac {1}{n}\sum_{t=1}^ne_t^2.
\label{mse}
\end{equation}

The \ac{MSE} is also known as a scale-dependent accuracy measure and measures the average squared deviation between the forecasted values and the real ones. The application of this measure is quite relevant because it doesn't allow the negative errors to cancel the positive errors and vice-versa. 

By applying the square root to the \ac{MSE}, the \ac{RMSE} is then defined as

\begin{equation}
     RMSE =\sqrt{MSE} = \sqrt{\frac {1}{n}\sum_{t=1}^ne_t^2}.
\label{rmse}
\end{equation}
Mathematically, the \ac{RMSE} is the square root of the average of the squared difference between the predicted values and the observed ones. The advantage of \ac{RMSE} over \ac{MSE} is that it is on the same scale as the targets, facilitating its interpretation in the concrete context of the problem. 

\ac{RMSE} and \ac{MAE} were the metrics implemented in the development of this research. These two measurements present some similarities, but also some differences that make it relevant to use the two metrics separately. In terms of similarities, both are expressed in the units of the variable in question, which means that both errors are easily interpretable given the context of the problem. The major difference between the two metrics lies in the contribution that individual error values make to the final result. In the case of \ac{MAE}, the contribution of individual errors follows a linear behavior. An error of 20 units contributes twice as much as an error of 10 units, and an error of 500 units contributes half as much as an error of 1000 units. In the case of \ac{MSE} the errors are squared before being averaged, which means that this metric gives more weight to larger errors.  This means that very small errors tend to be ignored while large errors tend to be magnified. One can then conclude that \ac{MSE} is especially useful when one intends to emphasize large errors and reduce the importance of smaller error. This is why this was the metric chosen as loss\_function, since by emphasizing large-scale errors, they end up being penalized, which is quite useful in time series, where the intention is to avoid this type of failure as much as possible. It was also decided to calculate, for all training, validations and tests, the \ac{MSE} and the \ac{MAE}. The first one because it translates the \ac{MSE} to the scale of the problem, allowing a better interpretation, and the second one because, given its nature, it can be useful in the evaluation of the results obtained.

\section{Conclusion}\label{chap3:sec:conclusion}

In this chapter, we presented the case-study of the problem. In the section \ref{chap3:sec:building}, we introduced the building used in the context of the problem, its characteristics and particularities. Then, in the section \ref{chap3:sec:variable_to_predict}, we presented two proposals of variables to predict, and one of the approaches was selected. In the section \ref{chap3:sec:data}, we presented all the available data and referred the whole process done on this data so that it could be used in the context of the problem.  In the section \ref{chap3:subsec:artificial_neural_networks} we also presented the models sugested to solve the problem proposed by \ac{EDP}, their specifications, the motivation behind their use, and the mechanisms used during their training and validation process. In the section \ref{chap3:sec:implementation_enviromnet} we explored the environment used to implement the proposed models, and finally, in the section \ref{chap3:sec:performance_evaluation_metrics}, we presented the metrics used to evaluate the performance of the models.